---
title: "Wave 2 pilot survey exploration"
author: "Aleksi Suuronen"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    lightbox: True
    thumbnails: True
    gallery: True
    toc_depth: 3
---

```{r include=FALSE}
#This chunk is for loading packages and setting up the datasets

library(labelled) # for attaching variable labels
library(stringr)
library(likert) # For plotting and analyzing likert Qs
library(ggplot2)
library(ggcorrplot) # For nice correlation matrix plots
library(ltm) # For calculating Cronbachâ€™s Alpha
library(stats)
library(psych) # For conducting factor analysis
library(gridExtra)
library(rmdformats) # for nice themes

#Loading the dataframes
DF1 <- read.csv("~/WORK/Bubble survey eOpinion Panel/FB 250 Pilot Data/Wave 2 default survey.csv", sep=";", comment.char="#")
DF2 <- read.csv("~/WORK/Bubble survey eOpinion Panel/FB 250 Pilot Data/Wave 2 alternative survey.csv", sep=";", comment.char="#")


#Removing the unnecessary row from DFs
lable_1 <- as.character((DF1[1,])) 
lable_2 <- as.character((DF2[1,])) 

DF1 <- DF1[-1,]
DF2 <- DF2[-1,]


#Setting variable labels
for(i in 1:ncol(DF1)){
  var_label(DF1[i]) <-  lable_1[i]
}

for(i in 1:ncol(DF2)){
  var_label(DF2[i]) <-  lable_2[i] 
}


```

Ok, lets take a closer look at the two pilot surveys we sent shall we? 

In case you need an reminder, you can get the surveys in a word version via these links I have put on Google drive (you probably need to download them since Google docs does not seem to be able to preview them)

* [Default Survey](https://docs.google.com/document/d/1yEQd7uvGu9cta69xfIh2zo5OEHZkBo0F/edit?usp=sharing&ouid=115594560134005060036&rtpof=true&sd=true)
  - The default survey had 70 respondents
* [Alternative Survey](https://docs.google.com/document/d/1whCGuNBsbSHtQ_x0RDG1t1HMJqQPoJ0d/edit?usp=sharing&ouid=115594560134005060036&rtpof=true&sd=true) 
  - The alternative survey had 83 respondents.

Lets first just compare these two versions in terms how long it took the respondents to fill them out:

```{r echo=FALSE}

paste("DEFAULT SURVEY: mean completion time in min:", round(mean(as.integer(DF1$Duration_.in_seconds.)/60),2))
paste("DEFAULT SURVEY: median completion time in min:", round(median(as.integer(DF1$Duration_.in_seconds.)/60),2))
paste("ALTERNATIVE SURVEY: mean completion time in min:", round(mean(as.integer(DF2$Duration_.in_seconds.)/60),2))
paste("ALTERNATIVE SURVEY: median completion time in min:", round(median(as.integer(DF2$Duration_.in_seconds.)/60),2))

```

Looks like there is not a huge difference in the median completion times. In both cases, the most common completion time was about 14 min. The mean completion times are out of wack, since some respondents probably left the session open and did not return to it or stopped filling out the survey, and returned to it much later (like 13 hours after).


## Feedback

Lets now take a look at the feedback and see whether differences exist between the surveys. 

First, here is the overall distribution to our Feedback questions:

```{r echo=FALSE, fig.height=4, fig.width=11, warning=FALSE}

#Getting the Feedback questions

Feedback_1 <- DF1[,c("FEEDBACK_1_1", "FEEDBACK_1_2", "FEEDBACK_1_3", "FEEDBACK_1_4", "FEEDBACK_1_5")]
Feedback_1$Survey_Version <- "Default version"

Feedback_2 <- DF2[,c("FEEDBACK_1_1", "FEEDBACK_1_2", "FEEDBACK_1_3", "FEEDBACK_1_4", "FEEDBACK_1_5")]
Feedback_2$Survey_Version <- "Alternative version"

FEEDBACK <- rbind(Feedback_1, Feedback_2)


names <-var_label(DF1[,c("FEEDBACK_1_1", "FEEDBACK_1_2", "FEEDBACK_1_3", "FEEDBACK_1_4", "FEEDBACK_1_5")])

names(FEEDBACK) <- names
names(FEEDBACK)[6] <- "Survey_Version"

for (i in 1:5) {
  
  FEEDBACK[i][FEEDBACK[i] == ""] <- NA
  FEEDBACK[,i] <- factor(FEEDBACK[,i], levels = c("Strongly disagree", "Somewhat disagree", "Neither agree nor disagree", "Somewhat agree", "Strongly agree"))
  
  
}


likert_Qs <- likert(FEEDBACK[1:5])
plot(likert_Qs, ordered=FALSE)

```

Next, here are the same questions divided by the survey versions. In addition, I added Chi square tests to see if the observed differences were statistically significant. 

```{r echo=FALSE, fig.height=7, fig.width=9, warning=F}

for (i in 1:5) {
  
  chi_test <- chisq.test(FEEDBACK[,i], FEEDBACK[,6])
  names(FEEDBACK)[i] <- paste(names[i], " [Chi2 p-value = ", round(chi_test$p.value, 2), "]", sep = "")
  
}

likert_Qs_G <- likert(FEEDBACK[1:5], grouping=FEEDBACK$Survey_Version)
plot(likert_Qs_G, ordered=FALSE)
```

While the p-values do not indicate significant differences, it looks like the default survey is consistently the preferred version:

* Respondents who took the Default version were 1.3 times more likely to say that it did not take too long
* Default survey respondents also did not feel as much that the questions were too complex
* They also had easier time to recall situations that the survey asked about

So, while the differences are not huge and not statistically significant, the consistency of the results do make me prefer the Default version.


## Default survey: constructing a Filter Bubble Index (FBI)

### Encountering agreeable and disagreeable content

Next up, lets look how the following questions look in the Default survey version:

**Disagreement:**

>When I encounter content about politics or other societal issues in my day-to-day life, I find myself disagreeing with it.

**Agreement:**

>When I encounter content about politics or other societal issues in my day-to-day life, I find myself agreeing with it.

First, I am just going to look at the basic distribution:

```{r echo=FALSE, fig.height=4, fig.width=11}

TEMP <- DF1[,c("II_Q1", "ORIE_Q1")]

for (i in 1:2) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  
  
}

names(TEMP) <- c("It is common for me to encounter political content I DISAGREE with", "It is common for me to encounter political content I AGREE with")

likert_Qs <- likert(TEMP[1:2])
plot(likert_Qs, ordered=FALSE)


```

Hmm, seems like encountering content that a person disagrees with is slightly more common than encountering agreeable content (67% for disagreement v. 53% for agreement)

But do the same respondent provide different answers to these questions, or do they report that the commonality of which they encounter content that they disagree with differs form how commonly they encounter content that they agree with?

In essence, this is me trying to do a sort of a "Filter Bubble" index, where people who score higher on the index are much more likely to be surrounded with content that they agree with than with content they disagree with.

My first idea was to use a ratio for this, where the index would simply be:

* commonality of encountering agreeable content = AC
  + Re-code Extremely uncommon, Extremely common to integers with a range of [1,5]
* commonality of encountering disagreeable content = AD
  + Re-code Extremely uncommon, Extremely common to integers with a range of [1,5]
* Filter Bubble Index (FBI) = AC/AD

Lets take a look what it would look like in the case of our sample:


```{r echo=FALSE}

TEMP$FBI <- as.numeric(TEMP[,2])/as.numeric(TEMP[,1])

agreement <- sum(TEMP$FBI > 1)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 1)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 1)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")

ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 40) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  annotate("segment", x = 0, xend = 1, y = 20, yend = 20,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
  annotate("segment", x = 1, xend = 3, y = 20, yend = 20,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
  annotate("label", x = 0.5, y = 20, size = 3, alpha = 0.5, fill = "red", label = disagreement) +
  annotate("label", x = 2, y = 20, size = 3, alpha = 0.5, fill = "green",  label = agreement) +
  xlab("FBI") +
  ylab("Count") +
  theme_bw()


```

In this case, we can see the following:

* 38 respondents (54%) have perfect balance, and encounter agreeable and disagreeable content the same amount
* 10 respondents (14%) are more likely to encounter agreeable content, so they are the ones we would consider to be in a filter bubble where the respondent who scored 2.5 is in the most thigh night bubble. 
* 22 respondents (31%) are more likely to encounter disagreeable content, so these are the ones who have "broken" the filter bubble.

While the ratio-method works at least somewhat, it is a bit annoying that the ratio scale introduces problems with scale. Once you go FBI < 0, then the distributions become much more tight night, which might be cumbersome to handle in possible regressions.

Of course we could just re-scale the FBI, but I think its just easier to work out a alternative way that does not use the ratio.

I think my preferred method is just to use negative and positive numbers and sum them up to get a similar index, but which does not have this scaling problem:

* commonality of encountering agreeable content = AC
  + Re-code Extremely uncommon, Extremely common to integers with a range of [1,5]
* commonality of encountering disagreeable content = AD
  + Re-code Extremely uncommon, Extremely common to integers with a range of [-1, -5]
* Filter Bubble Index (FBI) = AD + AC

So, encountering disagreeable content receives negative numbers in according to how commonly they are encountered, and then we will just sum AC and AD together, where_

* FBI = 0 = "Perfect balance"
* FBI > 0 = "More likely to encounter agreeable content, is in a filter bubble"
* FBI < 0 = "More likely to encounter disagreeable content, is not in a filter bubble"

This basically returns similar distributions as the figure above, but which is more balanced on either side of the 0 value:


```{r echo=FALSE}
TEMP$FBI <- as.numeric(TEMP[,1])*-1 + as.numeric(TEMP[,2])

agreement <- sum(TEMP$FBI > 0)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 0)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 0)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")

ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 40) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  annotate("segment", x = 0.5, xend = 5, y = 20, yend = 20,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
    annotate("segment", x = -5, xend = -0.5, y = 20, yend = 20,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
    annotate("segment", x = -0.5, xend = 0.5, y = 20, yend = 20,  colour = "white", size = 1, alpha = 0.2) +
  annotate("label", x = 3, y = 20, size = 3, alpha = 0.5, fill = "green",  label = agreement) +
  annotate("label", x = -3, y = 20, size = 3, alpha = 0.5, fill = "red",  label = disagreement) +
  annotate("text", x = 0, y = 21, size = 3, label = "Balanced", colour = "white") +
  xlab("FBI") +
  ylab("Count") +
  theme_bw()
```

From here on out, I will be using the summation-method for calculating the FBI.

### Encountering agreeable and disagreeable opinions from people

However, as you might recall, our theoretical framework also included encountering people who express opinions that the respondent agrees and disagrees with:

**Agreement:**

>When I encounter people discussing politics or other societal issues with each other, I find myself agreeing with the opinions that are being shared.

**Disagreement:**

>When I encounter people discussing politics or other societal issues with each other, I find myself disagreeing with the opinions that are being shared.

Lets first take a look at the basic distributions of these two questions:

```{r echo=FALSE, fig.height=4, fig.width=11}

TEMP <- DF1[,c("II_Q2", "ORIE_Q2")]

for (i in 1:2) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  
  
}

names(TEMP) <- c("When I encounter people discussing politics or other societal issues, I find myself DISAGREEING", "When I encounter people discussing politics or other societal issues, I find myself AGREEING")

likert_Qs <- likert(TEMP[1:2])
plot(likert_Qs, ordered=FALSE)


```

As opposed to the content question above, people seem to be just as likely to hear opinions that they disagree with from various people than they are to hear opinions they disagree with.

But are major differences here at the respondent level?

Here is the same FBI sum-calculation as above for these two questions:

```{r echo=FALSE}
TEMP$FBI <- as.numeric(TEMP[,1])*-1+as.numeric(TEMP[,2])

agreement <- sum(TEMP$FBI > 0)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 0)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 0)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")

ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 35) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  annotate("segment", x = 0.5, xend = 5, y = 20, yend = 20,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
    annotate("segment", x = -5, xend = -0.5, y = 20, yend = 20,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
    annotate("segment", x = -0.5, xend = 0.5, y = 20, yend = 20,  colour = "white", size = 1, alpha = 0.2) +
  annotate("label", x = 3, y = 20, size = 3, alpha = 0.5, fill = "green", label = agreement) +
  annotate("label", x = -3, y = 20, size = 3, alpha = 0.5, fill = "red", label = disagreement) +
  annotate("text", x = 0, y = 21, size = 3, label = "Balanced", colour = "white") +
  xlab("FBI") +
  ylab("Count") +
  theme_bw()

```

Compared with the content question it looks like the amount of "balanced" respondents is smaller and we have more people who we would consider to be in a filter bubble scenario:

* 33 respondents (47%) are "balanced"
* 16 respondents (23%) are in a filter bubble
* 21 respondents (30%) have "broken" the filter bubble

### The final FBI based on the default survey

We can now combine these two questions into a single FBI index by just again summing them together:

```{r echo=FALSE}


TEMP <- DF1[,c("II_Q1", "II_Q2", "ORIE_Q1", "ORIE_Q2")]

for (i in 1:4) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  
  
}

TEMP$FBI <- (as.numeric(TEMP[,1])*-1+as.numeric(TEMP[,2])*-1)+(as.numeric(TEMP[,3])+as.numeric(TEMP[,4]))

agreement <- sum(TEMP$FBI > 0)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 0)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 0)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")

p_default <- ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 27) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, size = 3) +
  annotate("segment", x = 0.5, xend = 5, y = 20, yend = 20,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
    annotate("segment", x = -7, xend = -0.5, y = 20, yend = 20,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.2) +
    annotate("segment", x = -0.5, xend = 0.5, y = 20, yend = 20,  colour = "white", size = 1, alpha = 0.2) +
  annotate("label", x = 3, y = 20, size = 3, alpha = 0.5, fill = "green", label = agreement) +
  annotate("label", x = -3, y = 20, size = 3, alpha = 0.5, fill = "red", label = disagreement) +
  annotate("text", x = 0, y = 21, size = 2, label = "Balanced", colour = "white") +
  xlab("FBI") +
  ylab("Count") +
  ggtitle("Final FBI based on the Default survey") +
  theme_bw()

p_default
```

Again, the number of people who are in the "balanced" category is reduced, but this time it is also quite clear, that it is more likely that the majority of your sample is not in a filter bubble, since they are more likely to encounter disagreeable content and opinions from people than they are to encounter agreeable versions of them:

* 25 respondents (36%) are "balanced"
* 20 respondents (29%) are in a filter bubble
* 25 respondents (36%) have "broken" the filter bubble

In addition, the proportions of these three categories become more even as we have combined these two question versions. 


## Alternative survey: constructing the second Filter Bubble Index (FBI)

### Encountering agreeable and disagreeable content

Ok, lets compare the above with the alternative version of the survey, where we the content agreement/disagreement questions were as follows:

**Disagreement**

>Overall, how common is it for you to encounter content about politics or other societal issues that you find yourself disagreeing with in the following situations?

**Agreement**

>Overall, how common is it for you to encounter content about politics or other societal issues that you find yourself agreeing with in the following situations?

And the basic distribution of this battery of questions is:

```{r echo=FALSE, fig.height=10, fig.width=10}

TEMP1 <- DF2[,c("II_Q1_1", "II_Q1_2", "II_Q1_3", "II_Q1_4", "II_Q1_5", "II_Q1_6", "II_Q1_7", "II_Q1_8", "II_Q1_9")]
TEMP1$Type <- "Disagreement"

TEMP2 <- DF2[,c("ORIE_Q1_1", "ORIE_Q1_2", "ORIE_Q1_3", "ORIE_Q1_4", "ORIE_Q1_5", "ORIE_Q1_6", "ORIE_Q1_7", "ORIE_Q1_8", "ORIE_Q1_9")]
TEMP2$Type <- "Agreement"
names(TEMP2) <- names(TEMP1)

TEMP <- rbind(TEMP1, TEMP2)

names <- as.character(var_label(TEMP))

for (i in 1:9) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  names[i] <- paste(names[i], " [No use = ", round((table(is.na(TEMP[,i]))[2]/length(TEMP[,i]))*100, 2), " %]", sep = "")

}

names(TEMP) <- names
names(TEMP)[10] <- "Type"

likert_Qs <- likert(TEMP[1:9], grouping=TEMP$Type)
plot(likert_Qs) + ggtitle("Encountering agreement and disagreement via various media")


```

Overall, it seems that Twitter is a place where people encounter content that they disagree with the most, but it is also worth noting that encountering content that a respondent agrees with is not all that uncommon either.

Looking at the numbers, what gets me a bit worried is the amount of "I do not use this media at all" answers (indicated as *No use* in the plot). It seems that only newspapers and TV are mediums which are used virtually by all of the respondents. 

This might have an impact on the results in that depending what media options we have listed in this question, then different kinds of people will shows as being in a filter bubble. In the default version of the survey, this is not as big of a problem since we ask the big "overall amount" people encounter content that they agree or disagree with. Here, we need to have a good list of different media options, or we might risk missing something.

but lets just see how my Filter Bubble Index (FBI) would work here. I am just going with my first idea here, and I will re-code all of the answers to a integers: 

 * encountering agreeable content: [1,5]
 * encountering disagreeable content [-1, -5]
 * "I don't use this media" is just a 0.
 
Then I will merely sum the answers together, which results in this distribution:

```{r echo=FALSE}

TEMP <- DF2[,c("ORIE_Q1_1", "ORIE_Q1_2", "ORIE_Q1_3", "ORIE_Q1_4", "ORIE_Q1_5", "ORIE_Q1_6", "ORIE_Q1_7", "ORIE_Q1_8", "ORIE_Q1_9", "II_Q1_1", "II_Q1_2", "II_Q1_3", "II_Q1_4", "II_Q1_5", "II_Q1_6", "II_Q1_7", "II_Q1_8", "II_Q1_9")]


for (i in 1:9) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

for (i in 10:18) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])*-1
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

TEMP$FBI <- NA
for (i in 1:nrow(TEMP)) {
  
  TEMP[i,"FBI"] <- sum(TEMP[i, c(1:18)])
  
}

agreement <- sum(TEMP$FBI > 0)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 0)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 0)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")


ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 10) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, size = 3) +
  annotate("label", x = 6, y = 5, size = 2.5, alpha = 0.4, label = agreement, fill = "Green") +
  annotate("label", x = -8, y = 5, size = 2.5, alpha = 0.4, label = disagreement, fill = "Red") +
  annotate("segment", x = 0.5, xend = 10, y = 5, yend = 5,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.4) +
    annotate("segment", x = -16, xend = -0.5, y = 5, yend = 5,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.4) +
    annotate("segment", x = -0.5, xend = 0.5, y = 5, yend = 5,  colour = "white", size = 1, alpha = 1) +
  xlab("FBI") +
  ylab("Count") +
  theme_bw()


```

As the figure shows, about 47 % are more likely to encounter agreement and 43 % are more likely to encounter disagreement.

This is quite a big change to the default version of the survey, where the wast majority were in the "balanced option. This might be a good thing, since this gives us more variation to work with.

### Encountering agreeable and disagreeable opinions from people

But lets look at the second part, which was about encountering OPINIONS from PEOPLE that the respondent either agrees or disagrees with:


**Disagreement:**

>Overall, how common is it for you to encounter opinions about politics or other societal issues that you find yourself disagreeing with in the following social situations?

**Agreement:**

>Overall, how common is it for you to encounter opinions about politics or other societal issues that you find yourself agreeing with in the following social situations?

```{r echo=FALSE, fig.height=8, fig.width=10}

TEMP1 <- DF2[,c("II_Q2_1", "II_Q2_2", "II_Q2_3", "II_Q2_4", "II_Q2_5", "II_Q2_6")]
TEMP1$Type <- "Disagreement"

TEMP2 <- DF2[,c("ORIE_Q2_1", "ORIE_Q2_2", "ORIE_Q2_3", "ORIE_Q2_4", "ORIE_Q2_5", "ORIE_Q2_6")]
TEMP2$Type <- "Agreement"
names(TEMP2) <- names(TEMP1)

TEMP <- rbind(TEMP1, TEMP2)

names <- as.character(var_label(TEMP))

for (i in 1:6) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  names[i] <- paste(names[i], " [NAs = ", round((table(is.na(TEMP[,i]))[2]/length(TEMP[,i]))*100, 2), " %]", sep = "")

}

names(TEMP) <- names
names(TEMP)[7] <- "Type"

likert_Qs <- likert(TEMP[1:6], grouping=TEMP$Type)
plot(likert_Qs) + ggtitle("Encountering agreement and disagreement via various media")


```

Here we can see pretty big differences. Friends in particular has a huge difference in how much a person hears opinions that he/she agrees with. Encountering agreeable opinions is quite clearly way more common.

The situation is completely reversed in politicians, which is not all that suprising.

But lets see how the FBI would look like in this case:


```{r echo=FALSE, warning=FALSE}

TEMP <- DF2[,c("ORIE_Q2_1", "ORIE_Q2_2", "ORIE_Q2_3", "ORIE_Q2_4", "ORIE_Q2_5", "ORIE_Q2_6", "II_Q2_1", "II_Q2_2", "II_Q2_3", "II_Q2_4", "II_Q2_5", "II_Q2_6")]


for (i in 1:6) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

for (i in 7:12) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])*-1
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

TEMP$FBI <- NA
for (i in 1:nrow(TEMP)) {
  
  TEMP[i,"FBI"] <- sum(TEMP[i, c(1:12)])
  
}

agreement <- sum(TEMP$FBI > 0)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 0)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 0)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")


ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 10) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, size = 3) +
  annotate("label", x = 5, y = 5, size = 2.5, alpha = 0.6, label = agreement, fill = "Green") +
  annotate("label", x = -6, y = 5, size = 2.5, alpha = 0.6, label = disagreement, fill = "Red") +
  annotate("segment", x = 0.5, xend = max(TEMP$FBI), y = 5, yend = 5,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.4) +
    annotate("segment", x = min(TEMP$FBI), xend = -0.5, y = 5, yend = 5,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.4) +
    annotate("segment", x = -0.5, xend = 0.5, y = 5, yend = 5,  colour = "white", size = 1, alpha = 1) +
  xlab("FBI") +
  ylab("Count") +
  theme_bw()



```

Looks like things now lean more clearly towards people encountering opinions that they agree with. This is more common for the majority of the respondents.

### The final FBI based on the alternative survey

But what happens when we combine these two questions together and how does it compare to the default survey?


```{r fig.height=5, fig.width=12, echo=FALSE}

TEMP <- DF2[,c("ORIE_Q1_1", "ORIE_Q1_2", "ORIE_Q1_3", "ORIE_Q1_4", "ORIE_Q1_5", "ORIE_Q1_6", "ORIE_Q1_7", "ORIE_Q1_8", "ORIE_Q1_9", "ORIE_Q2_1", "ORIE_Q2_2", "ORIE_Q2_3", "ORIE_Q2_4", "ORIE_Q2_5", "ORIE_Q2_6", "II_Q1_1", "II_Q1_2", "II_Q1_3", "II_Q1_4", "II_Q1_5", "II_Q1_6", "II_Q1_7", "II_Q1_8", "II_Q1_9", "II_Q2_1", "II_Q2_2", "II_Q2_3", "II_Q2_4", "II_Q2_5", "II_Q2_6")]



for (i in 1:15) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

for (i in 16:30) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])*-1
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

TEMP$FBI <- NA
for (i in 1:nrow(TEMP)) {
  
  TEMP[i,"FBI"] <- sum(TEMP[i, c(1:30)])
  
}

agreement <- sum(TEMP$FBI > 0)
agreement <- paste(agreement, " (", round(agreement/nrow(TEMP)*100,2), "%)\nAgreement more likely", sep = "")
disagreement <- sum(TEMP$FBI < 0)
disagreement <- paste(disagreement, " (", round(disagreement/nrow(TEMP)*100,2), "%)\nDisagreement more likely", sep = "")
balanced <- sum(TEMP$FBI == 0)
balanced <- paste(balanced, " (", round(balanced/nrow(TEMP)*100,2), "%)\n'Balanced'", sep = "")


p_alternative <-ggplot(TEMP, aes(x=FBI))+
  geom_bar() +
  ylim(0, 10) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, size = 3) +
  annotate("label", x = 10, y = 0.5, size = 2.5, alpha = 0.8, label = agreement, fill = "Green") +
  annotate("label", x = -10, y = 0.5, size = 2.5, alpha = 0.8, label = disagreement, fill = "Red") +
  annotate("segment", x = 0.5, xend = max(TEMP$FBI), y = 0.5, yend = 0.5,  colour = "green", size = 1, arrow=arrow(ends = "both"), alpha = 0.4) +
    annotate("segment", x = min(TEMP$FBI), xend = -0.5, y = 0.5, yend = 0.5,  colour = "red", size = 1, arrow=arrow(ends = "both"), alpha = 0.4) +
    annotate("segment", x = -0.5, xend = 0.5, y = 0.5, yend = 0.5,  colour = "white", size = 1, alpha = 1) +
  xlab("FBI") +
  ylab("Count") +
  ggtitle("Final FBI based on the Alternative survey") +
  theme_bw()

grid.arrange(p_default, p_alternative, nrow = 1)

```

Right, the net-result seems to be that agreement is overall more common for most of the respondents in the Alternative survey.

In contrast, in the default survey disagreement seems to be more common overall.

Also, the role of "balanced" respondents is drastically different. In the default survey they are as large as those who are more likely to encounter disagreement. In the alternative version, on the other hand, they are virtually non-existent. 

## Affective polarization and the two FBIs?

Now, I wonder if there are differences between these two FBIs in how they are connected with affective polarization. Lets just check this first before taking any strong conclusions on which one of these metrics is "better" for our purposes.

I am going to load the survey from the first pilot that we conducted, which included the same people who responded to our current pilot.

I will first calculate a simple measure for affective polarization by using the like-dislike scores and assigning ingroups based on voting behavior: the party the respondent indicated voting in the last parliamentary elections is the inparty. All other parties are considered as outparties.

Then, AP was measured for each respondent by taking their inpartys like-dislike score and subtracting that from each of the outparties the respondent ranked. The final affective polarization metric is then the average of those distances.

The distribution of this AP measure looks like this:

```{r echo=FALSE, message=FALSE, warning=FALSE}

DF_W1 <- read.csv("C:/Users/aleks/Documents/WORK/Bubble survey eOpinion Panel/FB 250 Pilot Data/Wave 1 pilot (Cleaned).csv", sep=";", comment.char="#")

DF_W1 <- DF_W1[,c("ResponseId", "POL_vote_2", "PARTY_EVAL_1", "PARTY_EVAL_2", "PARTY_EVAL_3", "PARTY_EVAL_4", "PARTY_EVAL_5", "PARTY_EVAL_6", "PARTY_EVAL_7", "PARTY_EVAL_8", "PARTY_EVAL_9")]

names(DF_W1)[3:11] <- c("THERMO_SDP", "THERMO_KOK", "THERMO_PS", "THERMO_KESK", "THERMO_VIHR", "THERMO_VAS", "THERMO_RKP", "THERMO_KD", "THERMO_NYT")

names(DF_W1)[2] <- "IN_PARTY"


DF_W1$IN_PARTY <- factor(DF_W1$IN_PARTY, levels = c("Suomen Sosialidemokraattinen Puolue (SDP)", "Kansallinen Kokoomus (Kok.)", "Perussuomalaiset (PS)", "Suomen Keskusta (Kesk.)", "VihreÃ¤ liitto (Vihr.)", "Vasemmistoliitto (Vas.)", "Suomen ruotsalainen kansanpuolue (RKP)", "Suomen Kristillisdemokraatit (KD)", "Liike Nyt (Liik.)", "Muu, mikÃ¤?"), labels = c("SDP", "KOK", "PS", "KESK", "VIHR", "VAS", "RKP", "KD", "NYT", "MUU"))

DF_W1$API <- NA
PARTIES <- levels(DF_W1$IN_PARTY)

for (i in 1:nrow(DF_W1)) {
  
  IN_PARTY <- as.character(DF_W1[i,"IN_PARTY"])
  OUT_PARTIES <- subset(PARTIES, PARTIES != IN_PARTY)
  
  API <- NULL
  for (party in OUT_PARTIES) {
    
    distance <- DF_W1[i,c(paste("THERMO_", IN_PARTY, sep = ""))] - DF_W1[i,c(paste("THERMO_", party, sep = ""))]

    
    API <- append(API, distance)
    
  }
  
  API <- na.omit(API)
  API <- sum(API)/length(API)
  DF_W1[i, "API"] <- API
  
}


ggplot(DF_W1, aes(API)) +
  geom_histogram(aes(y = ..density..), colour = 1, fill = "white") +
  geom_density(lwd = 1, colour = 4, fill = 4, alpha = 0.25) +
  theme_bw()


```

Ok, lets see if this correlates with the constructed FBI measures above.

First up, lets look at the FBI constructed from the default survey. I am testing both Pearsons correlation and Kendal Tau correlations just in case:


```{r echo=FALSE, message=FALSE, warning=FALSE}

TEMP <- DF1[,c("ResponseId",  "ORIE_Q1", "ORIE_Q2", "II_Q1", "II_Q2")]

for (i in 2:3) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])
  
  
}

for (i in 4:5) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])*-1
  
  
}

TEMP$FBI <- NA
for (i in 1:70) {
  
  TEMP[i,"FBI"] <- sum(TEMP[i,c(2:5)])
  
}

TEMP <- merge(TEMP, DF_W1, by="ResponseId")

ggplot(TEMP, aes(FBI, API))+
  geom_point() +
  geom_smooth(method=lm) +
  theme_bw()

cor.test(TEMP$API, TEMP$FBI, method = "pearson")
cor.test(TEMP$API, TEMP$FBI, method = "kendall")
```

OK, interesting. Looks like the relationship is negative, which would go against the filter bubble theory. The assumption was that those who most resemble being in a filter bubble where they are more likely to just hear information they agree with would be more polarized than those who are exposed to the other side more often.

Although the relationship is significant at 0.05 level only with the Pearsons correlation and not with Kendall Tau's.

Lets see if the situation is similar in the alternative survey


```{r echo=FALSE, message=FALSE, warning=FALSE}

TEMP <- DF2[,c("ResponseId", "ORIE_Q1_1", "ORIE_Q1_2", "ORIE_Q1_3", "ORIE_Q1_4", "ORIE_Q1_5", "ORIE_Q1_6", "ORIE_Q1_7", "ORIE_Q1_8", "ORIE_Q1_9", "ORIE_Q2_1", "ORIE_Q2_2", "ORIE_Q2_3", "ORIE_Q2_4", "ORIE_Q2_5", "ORIE_Q2_6", "II_Q1_1", "II_Q1_2", "II_Q1_3", "II_Q1_4", "II_Q1_5", "II_Q1_6", "II_Q1_7", "II_Q1_8", "II_Q1_9", "II_Q2_1", "II_Q2_2", "II_Q2_3", "II_Q2_4", "II_Q2_5", "II_Q2_6")]

for (i in 2:16) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

for (i in 17:31) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Extremely uncommon", "Fairly uncommon", "Neither common nor uncommon", "Fairly common", "Extremely common"))
  TEMP[,i] <- as.numeric(TEMP[,i])*-1
  TEMP[,i][is.na(TEMP[,i])] <- 0
  
}

TEMP$FBI <- NA
for (i in 1:nrow(TEMP)) {
  
  TEMP[i,"FBI"] <- sum(TEMP[i, c(2:31)])
  
}

TEMP <- merge(TEMP, DF_W1, by="ResponseId")


ggplot(TEMP, aes(FBI, API))+
  geom_point() +
  geom_smooth(method=lm) +
  theme_bw()

cor.test(TEMP$API, TEMP$FBI, method = "pearson")
cor.test(TEMP$API, TEMP$FBI, method = "kendall")

```

Ok, the relationship is still negative, but no longer significant in either of the correlations.

My tenative thoughts from all of this is that perhaps the default survey does a bit better job in diffirienting people into three clear camps, where as the alternative survey provides more variation, but also more noice to the variable.

In the default survey, we are pretty clear in asking OVERALL how much people encounter agreement and disagreement in their day-today lives, so it is perhaps a bit easier to give the "gut feeling" answer. My initial worry with this was that most people would just provide the same answer for both agreement and disagreement versions of the questions, but I don't think that this is a bit worry anymore. When I combined the two questions (encountering content and encountering opinions), it seems that we will get roughly 3 different populations: 1) those who clearly say they that it is more common for them to encounter disagreement, those who will provide the same answer to the agreement / disagreement versions (these would be the "balanced" population), and 3) those who clearly think they encounter more agreement than disagreement.

In the alternative version, we are loosing the middle group and rather have different degrees of "more likely to encounter agreement" and "more likely to encounter disagreement". But, I dont think this is as informative as the default version. Since we are asking the same question so many times again and again, we are bound to get more variation, but we also are loosing the nice and simple "gut feeling" answer. People might start answering this question more haphazardly to ease the cumbersomeness of the question battery, which leads to "noisy" answers.


## Sympathetic and antagonistic relationships

One more thing before I summarize what my thoughts are for the final survey based on this pilot. I want to take a closer look at the sympathetic and antagonistic question batteries since it may be true that the battery does not differentiate between antagonism and symphathisim clearly enough.

As a reminder, here is the actual question:

**Discussions with a person who SHARES the same values:**

> When I do have discussions about politics or other societal issues with a person who I think does share the same basic values as I do, I often find that these discussions tend to turn intoâ€¦

**Discussions with a person who DOES NOT SHARE the same values:**

> When I do have discussions about politics or other societal issues with a person who I think did not share the same basic values as I do, I often find that these discussions tend to turn intoâ€¦

Since we did not have different versions of these question, I will just combine the two surveys together, getting a total sample size of **153**

Here are the basic distributions:

```{r echo=FALSE, fig.height=9, fig.width=11}

labels <- as.character(var_label(DF1[,c("SYRHON_Q2_1", "SYRHON_Q2_2", "SYRHON_Q2_3", "SYRHON_Q2_4", "SYRHON_Q2_5", "SYRHON_Q2_6", "SYRHON_Q2_7", "SYRHON_Q2_8")]))

labels[1:4] <- paste(labels[1:4], "[SYMPATHETIC]")
labels[5:8] <- paste(labels[5:8], "[ANTAGONISTIC]")

TEMP1 <- DF1[,c("SYRHON_Q2_1", "SYRHON_Q2_2", "SYRHON_Q2_3", "SYRHON_Q2_4", "SYRHON_Q2_5", "SYRHON_Q2_6", "SYRHON_Q2_7", "SYRHON_Q2_8")]
names(TEMP1) <- labels
TEMP1$Version <- "Version 1"
TEMP1$Shared_value <- "DOES Share the same basic values"


TEMP2 <- DF2[,c("SYRHON_Q2_1", "SYRHON_Q2_2", "SYRHON_Q2_3", "SYRHON_Q2_4", "SYRHON_Q2_5", "SYRHON_Q2_6", "SYRHON_Q2_7", "SYRHON_Q2_8")]
names(TEMP2) <- labels
TEMP2$Version <- "Version 2"
TEMP2$Shared_value <- "DOES Share the same basic values"

TEMP <- rbind(TEMP1, TEMP2)

TEMP1 <- DF1[,c("ANRHEN_Q2_1","ANRHEN_Q2_2","ANRHEN_Q2_3","ANRHEN_Q2_4","ANRHEN_Q2_5","ANRHEN_Q2_6","ANRHEN_Q2_7","ANRHEN_Q2_8")]
names(TEMP1) <- labels
TEMP1$Version <- "Version 1"
TEMP1$Shared_value <- "DOES NOT Share the same basic values"


TEMP2 <- DF2[,c("ANRHEN_Q2_1","ANRHEN_Q2_2","ANRHEN_Q2_3","ANRHEN_Q2_4","ANRHEN_Q2_5","ANRHEN_Q2_6","ANRHEN_Q2_7","ANRHEN_Q2_8")]
names(TEMP2) <- labels
TEMP2$Version <- "Version 2"
TEMP2$Shared_value <- "DOES NOT Share the same basic values"

TEMP <- rbind(TEMP, TEMP1, TEMP2)


for (i in 1:8) {
  
  TEMP[i][TEMP[i] == ""] <- NA
  TEMP[,i] <- factor(TEMP[,i], levels = c("Strongly disagree", "Somewhat disagree", "Neither agree nor disagree", "Somewhat agree", "Strongly agree"))
  
  
}


likert_Qs_G <- likert(TEMP[,c(1:8)], grouping=TEMP$Shared_value)
plot(likert_Qs_G, ordered=T)

```

Seems like there are some clear differences in the answers depending if the conversation has been done with a person who does or does not share the values of the respondent.

In particular, notable differences can be seen in these two questions:

* pleasant experiences that made me appreciate the other person more
* productive debates where I learned something new 
* unproductive quarreling that have left a sour taste in my mouth


### Cronbach's alpha and a Factor analysis

But lets take a look at whether the two dimensions we are trying to capture here have internal consistency: how closely the items that measure antagonistic and sympathetic relations are related as a group.

First, lets first construct a correlation matrix on these item. The plot below conducted pearson correlations on the items, and all insignificant correlations are blanked out from the boxes:

```{r echo=FALSE}


for (i in 1:8) {
  
  TEMP[,i] <- as.numeric(TEMP[,i])
  
}

TEMP <- subset(TEMP, complete.cases(TEMP[1:8])) 

names(TEMP)[1:4] <- paste("V_", 1:4, " [SYM]", sep = "")
names(TEMP)[5:8] <- paste("V_", 5:8, " [ANT]", sep = "")
corr <- round(cor(TEMP[1:8]), 1)
p.mat <- cor_pmat(TEMP[1:8])

ggcorrplot(corr, hc.order = F, type = "lower", lab = T, p.mat = p.mat, insig = "blank")
```

* **V_1 [SYM]:** productive debates where I learned something new                   
* **V_2 [SYM]:** pleasant experiences that made me appreciate the other person more
* **V_3 [SYM]:** passionate debates that I enjoyed but which ultimately did not change my opinions
* **V_4 [SYM]:** educational experiences which have made me re-consider some of my opinions
* **V_5 [ANT]:** unproductive quarreling that have left a sour taste in my mouth
* **V_6 [ANT]:** delicate situations in which I may have hurt the other persons feelings
* **V_7 [ANT]:** uncomfortable situations where I felt personally attacked due to the opinions that I have
* **V_8 [ANT]:** thrilling verbal fights where I enjoyed arguing why some of my opinions are correct


Ok, all of the items that measure sympathetic relations are positively correlated with each other, which is what we would expect. However, almost all of the antagonistic measures are also positively correlated with each other except V_7 and V_8. In addition, V_8 is also positively correlated with ALL of the sympathetic items, which goes to show that perhaps this statement does not capture antagonism clearly enough.

Lets also calculate Cronbachâ€™s alpha for the two constructs to see how internally consistent the items are:


```{r echo=FALSE}

print("Sympathetic items")
cronbach.alpha(TEMP[1:4], standardized = T)
print("Antagonistic items")
cronbach.alpha(TEMP[5:8], standardized = T)

```

The first alpha sympathetic items is 0.725, which is within the [acceptable range](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/cronbachs-alpha-spss/).

The second alpha for the antagonistic items, and this fairs a bit poorer, falling below the conventionally acceptable range.

I think V_8 is the likely reason why this is happening, but lets also do a quick factor analysis to see to what factor loading that item gets.

First, lets do a graph to check whether we indeed have only two dimensions here, and if this is the case then do factor analysis with two components with a varimax rotation.

```{r echo=FALSE}
fit <- princomp(TEMP[1:8], cor=TRUE)
plot(fit,type="lines")

fit <- principal(TEMP[1:8], nfactors=2, rotate="varimax")
fit

```

Yep, as we would like to see, the principal component plot would suggest that we have two major components. When I conduct a factor analysis with two components, most of the item loading make sense. Our sympathic items have the biggest factor loading in the first component (RC1 in the output window), but V_8 also has a high loading there!

V_8 also does not have particularly high loading in the second component (RC2) where all of the other antagonistic items are.

So, it looks like it would be a good idea to re-formulate this item a bit differently, so that it captures antagonisim more clearly. 

## Conclusions:

To sum up, I think our next steps should be

1. Stick with the default version
2. re-formulate the following item:

> V_8: thrilling verbal fights where I enjoyed arguing why some of my opinions are correct

While the alternative survey does give us more variation to work with in the agreement/disagreement questions, I have a suspicion that that variation will not end up being as informative as what we get in the default version. In the default version we seem to get three very distinct populations who have clearly indicated whether they think that they are more likely to encounter agreeable or disagreeable content or whether they think they encounter them pretty much the same amount.

Moreover, it seems that the default survey is also easier for the respondents to answer to, and having one big OVERALL question about encountering agreeable / disagreeable information is easier for us to work with analytically as well, since we don't have to worry about whether we have included sufficient options for different media channels or different people. We also dot have to worry about to what degree people do not use different media options that we have listed.

However, I will try to modify the V_8 antagonistic item, and perhaps also V_3. These options are highly correlated with each other, and they perhaps blur the line between antagonism and sympathisim too much, but I will get back to you on this once I have thought about different options.


